<div align="center">

# âœ¦ Ollama Chat Interface

**A sleek, local AI chat app built with Python & CustomTkinter**  
**Una app de chat IA local, hecha con Python y CustomTkinter**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=flat-square&logo=python)](https://python.org)
[![CustomTkinter](https://img.shields.io/badge/CustomTkinter-5.2.2-purple?style=flat-square)](https://github.com/TomSchimansky/CustomTkinter)
[![LangGraph](https://img.shields.io/badge/LangGraph-0.2.74-cyan?style=flat-square)](https://langchain-ai.github.io/langgraph/)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-green?style=flat-square)](https://ollama.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](LICENSE)

</div>

---

## ğŸ–¼ï¸ Screenshots

> Chat view with multi-session sidebar / Vista de chat con sidebar de sesiones mÃºltiples

<img width="1258" height="807" alt="image" src="https://github.com/user-attachments/assets/83f710dd-3957-4853-89e4-a58e26c75a37" />



---

## ğŸ‡¬ğŸ‡§ English

### What is this?

Ollama Chat Interface is a **local, private desktop app** that lets you chat with any AI model running on [Ollama](https://ollama.com) â€” think of it like a personal ChatGPT that runs entirely on your machine, no internet, no subscriptions, no data leaving your computer.

It's built with Python using **CustomTkinter** for the modern UI, **LangGraph** for conversation memory management, and the **Ollama** API for model inference.

### âœ¨ Features

- ğŸ—‚ï¸ **Multi-session chat** â€” create, switch, and delete independent conversations
- ğŸ¤– **Model selector** â€” pick any model installed in Ollama from the sidebar dropdown
- ğŸ’¬ **Bubble-style UI** â€” user messages on the right, AI on the left, just like a real messenger
- âœ¦ **Typing indicator** â€” animated neon dots while the model is thinking
- ğŸ“ **Basic Markdown rendering** â€” supports `**bold**`, `` `inline code` ``, and ` ```code blocks``` `
- ğŸ’¾ **Export chat** â€” save any conversation as `.md` or `.txt`
- ğŸ—‘ï¸ **Clear session** â€” wipe the history of any chat without losing others
- ğŸ§  **Persistent context** â€” uses LangGraph's `MemorySaver` to keep conversation history per session
- ğŸ¨ **Synthwave dark theme** â€” purple, cyan & neon pink palette built with CustomTkinter

### ğŸš€ Getting Started

**Requirements:**
- Python 3.10+
- [Ollama](https://ollama.com) installed and running locally

```bash
# 1. Clone the repo
git clone https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py.git
cd Chat-Interface-GUI-Ollama-Py

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start Ollama (in a separate terminal)
ollama serve

# 4. Pull a model if you don't have one yet
ollama pull llama3.2

# 5. Run the app
python main.py
```

### ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|---|---|
| `Python 3.10+` | Core language |
| `CustomTkinter 5.2.2` | Modern UI widgets |
| `LangGraph 0.2.74` | Conversation graph & memory |
| `LangChain Ollama` | Ollama model integration |
| `Requests` | Fetch available models from Ollama API |

---

## ğŸ‡ªğŸ‡¸ EspaÃ±ol

### Â¿QuÃ© es esto?

Ollama Chat Interface es una **app de escritorio local y privada** para chatear con cualquier modelo de IA que tengas en [Ollama](https://ollama.com). Es bÃ¡sicamente un ChatGPT personal que corre completamente en tu mÃ¡quina â€” sin internet, sin suscripciones, sin que tus datos salgan a ningÃºn lado.

EstÃ¡ hecha en Python con **CustomTkinter** para la interfaz moderna, **LangGraph** para manejar la memoria de la conversaciÃ³n, y la **API de Ollama** para la inferencia de modelos.

### âœ¨ Funcionalidades

- ğŸ—‚ï¸ **Multi-sesiÃ³n** â€” crea, cambia y elimina conversaciones independientes
- ğŸ¤– **Selector de modelo** â€” elige cualquier modelo instalado en Ollama desde el sidebar
- ğŸ’¬ **UI tipo messenger** â€” mensajes del usuario a la derecha, respuestas de la IA a la izquierda
- âœ¦ **Indicador de escritura** â€” puntos neÃ³n animados mientras el modelo piensa
- ğŸ“ **Markdown bÃ¡sico** â€” soporta `**negrita**`, `` `cÃ³digo inline` `` y ` ```bloques de cÃ³digo``` `
- ğŸ’¾ **Exportar chat** â€” guarda cualquier conversaciÃ³n como `.md` o `.txt`
- ğŸ—‘ï¸ **Borrar sesiÃ³n** â€” limpia el historial de un chat sin afectar los demÃ¡s
- ğŸ§  **Contexto persistente** â€” usa `MemorySaver` de LangGraph para mantener el historial por sesiÃ³n
- ğŸ¨ **Tema dark synthwave** â€” paleta de pÃºrpura, cyan y rosa neÃ³n con CustomTkinter

### ğŸš€ CÃ³mo usarlo

**Requisitos:**
- Python 3.10+
- [Ollama](https://ollama.com) instalado y corriendo localmente

```bash
# 1. Clona el repo
git clone https://github.com/JulianDataScienceExplorerV2/Chat-Interface-GUI-Ollama-Py.git
cd Chat-Interface-GUI-Ollama-Py

# 2. Instala las dependencias
pip install -r requirements.txt

# 3. Inicia Ollama (en otra terminal)
ollama serve

# 4. Descarga un modelo si no tienes ninguno
ollama pull llama3.2

# 5. Corre la app
python main.py
```

### ğŸ› ï¸ Stack tecnolÃ³gico

| Herramienta | Para quÃ© |
|---|---|
| `Python 3.10+` | Lenguaje principal |
| `CustomTkinter 5.2.2` | Widgets de UI modernos |
| `LangGraph 0.2.74` | Grafo de conversaciÃ³n y memoria |
| `LangChain Ollama` | IntegraciÃ³n con modelos Ollama |
| `Requests` | Obtener modelos disponibles desde la API de Ollama |

---

<div align="center">

Made with ğŸ’œ by [JulianDataScienceExplorerV2](https://github.com/JulianDataScienceExplorerV2)

*Running local AI, the way it should be.*

</div>
